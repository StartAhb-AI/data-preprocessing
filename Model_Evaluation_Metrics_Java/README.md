# ğŸ“Š Model Evaluation Metrics in Machine Learning

This document explains **Accuracy, Precision, Recall, F1 Score, and Confusion Matrix** with examples and Java code.  
These metrics help us measure how good a model is beyond just accuracy.

---

## ğŸ”¹ Why Evaluation Metrics?
- Accuracy alone is not always reliable.
- Models can perform differently on **imbalanced datasets**.
- We need better insights:
    - **Confusion Matrix**
    - **Precision**
    - **Recall**
    - **F1 Score**

---

## ğŸ”¹ Confusion Matrix
A table that compares **actual vs predicted** results.

|                   | Predicted Positive | Predicted Negative |
|-------------------|--------------------|--------------------|
| **Actual Positive** | True Positive (TP) | False Negative (FN) |
| **Actual Negative** | False Positive (FP) | True Negative (TN) |

ğŸ“Œ **Example**: Spam vs Not-Spam Emails

- TP = Correctly identified spam
- TN = Correctly identified normal emails
- FP = Normal email marked as spam (false alarm)
- FN = Spam email missed (undetected)

---

## ğŸ”¹ Accuracy
**Formula**:  
\[
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
\]

âœ… Meaning: â€œOverall correctnessâ€  
âš ï¸ Problem: Misleading if data is imbalanced.

---

## ğŸ”¹ Precision
**Formula**:  
\[
Precision = \frac{TP}{TP + FP}
\]

âœ… Meaning: â€œWhen the model predicts positive, how often is it correct?â€  
ğŸ“Œ Example: *When model says Spam, how often is it really Spam?*

---

## ğŸ”¹ Recall
**Formula**:  
\[
Recall = \frac{TP}{TP + FN}
\]

âœ… Meaning: â€œOut of all actual positives, how many did we catch?â€  
ğŸ“Œ Example: *Out of all Spam emails, how many did we detect?*

---

## ğŸ”¹ F1 Score
**Formula**:  
\[
F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
\]

âœ… Meaning: Balance between **Precision & Recall**  
ğŸ“Œ Useful when both false positives and false negatives are costly.

---

## ğŸ”¹ Java Example (Manual Calculation)
```java
public class EvaluationExample {
    public static void main(String[] args) {
        int TP = 15, TN = 70, FP = 10, FN = 5;

        double accuracy  = (double)(TP+TN)/(TP+TN+FP+FN);
        double precision = (double) TP / (TP+FP);
        double recall    = (double) TP / (TP+FN);
        double f1        = 2*(precision*recall)/(precision+recall);

        System.out.println("Accuracy: " + accuracy);
        System.out.println("Precision: " + precision);
        System.out.println("Recall: " + recall);
        System.out.println("F1 Score: " + f1);
    }
}


 Accuracy

Formula:

ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ =   ğ‘‡ğ‘ƒ+ğ‘‡ğ‘/ğ‘‡ğ‘ƒ+ğ‘‡ğ‘+ğ¹ğ‘ƒ+ğ¹ğ‘

Meaning: â€œOverall correctnessâ€

Works well if dataset is balanced

Problem: Misleading if one class dominates

 Precision

Formula:

ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›=ğ‘‡ğ‘ƒ/ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ


Meaning: â€œOf all predicted positives, how many are correct?â€

Example: When model says Spam, how often is it really Spam?

 Recall

Formula:

ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™=ğ‘‡ğ‘ƒ/ğ‘‡ğ‘ƒ+ğ¹ğ‘



Meaning: â€œOf all actual positives, how many did we catch?â€

Example: Out of all Spam emails, how many did we detect?

 F1 Score

Formula:
F1=2Ã—Precision*Recall/Precision+Recall

Meaning: Balance between Precision & Recall

Used when both false positives and false negatives are costly
